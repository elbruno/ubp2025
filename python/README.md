# Python con GitHub Copilot - Ciencia de Datos y Desarrollo Web

## üéØ Objetivos de Aprendizaje

Al completar esta secci√≥n, podr√°s usar GitHub Copilot para:
- An√°lisis de datos con pandas y numpy
- Crear APIs web con FastAPI
- Implementar modelos de Machine Learning
- Visualizaci√≥n de datos con matplotlib y seaborn
- Automatizaci√≥n de tareas con scripts Python

## üõ†Ô∏è Configuraci√≥n del Entorno

### Prerrequisitos
- **Python 3.9+** 
- **pip** para gesti√≥n de paquetes
- **Virtual environment** (venv o conda)
- **Visual Studio Code** con GitHub Copilot

### Configuraci√≥n Inicial
```bash
# Crear entorno virtual
python -m venv copilot_python_env

# Activar entorno (Windows)
copilot_python_env\Scripts\activate
# Activar entorno (macOS/Linux)
source copilot_python_env/bin/activate

# Instalar paquetes b√°sicos
pip install pandas numpy matplotlib seaborn scikit-learn fastapi uvicorn jupyter
```

### Verificaci√≥n
```python
# test_setup.py - Copilot puede generar este c√≥digo
# Script para verificar que todas las librer√≠as est√°n instaladas correctamente
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets
import fastapi

print("‚úÖ Todas las librer√≠as instaladas correctamente!")
```

## üìä Secci√≥n 1: An√°lisis de Datos con Pandas

### ü•á Ejercicio 1: An√°lisis de Ventas

#### Objetivo
Analizar datos de ventas usando pandas con ayuda de Copilot.

#### Dataset de Ejemplo
```python
# Crear dataset de ventas para pr√°ctica
# Dataset debe incluir: fecha, producto, categoria, precio, cantidad, vendedor, region
# Generar 1000 registros de datos sint√©ticos realistas
# Incluir fechas del √∫ltimo a√±o, productos variados, multiple regiones
```

#### Prompt para Copilot Chat:
```
Crea un DataFrame de pandas con datos sint√©ticos de ventas que incluya:
- 1000 registros
- Columnas: fecha (√∫ltimo a√±o), producto, categoria, precio, cantidad, vendedor, region
- Datos realistas para una empresa de tecnolog√≠a
- Guarda como CSV para reutilizar
```

#### An√°lisis Solicitado
```python
# An√°lisis exploratorio de datos de ventas:
# 1. Resumen estad√≠stico b√°sico de todas las columnas num√©ricas
# 2. Ventas totales por mes y regi√≥n
# 3. Top 10 productos m√°s vendidos
# 4. Vendedor con mejor performance
# 5. An√°lisis de estacionalidad en las ventas
# 6. Correlaci√≥n entre precio y cantidad vendida
```

#### Soluci√≥n de Referencia

<details>
<summary>Ver soluci√≥n completa</summary>

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import random

# Generar datos sint√©ticos
def generar_datos_ventas():
    """Genera dataset sint√©tico de ventas para an√°lisis."""
    np.random.seed(42)
    
    # Configurar par√°metros
    n_records = 1000
    start_date = datetime.now() - timedelta(days=365)
    
    # Listas de datos
    productos = ['iPhone 14', 'Samsung Galaxy', 'iPad', 'MacBook Pro', 'Surface Pro', 
                'AirPods', 'Xbox Series X', 'PlayStation 5', 'Nintendo Switch']
    categorias = ['Smartphones', 'Tablets', 'Laptops', 'Audio', 'Gaming']
    vendedores = ['Ana Garc√≠a', 'Carlos L√≥pez', 'Mar√≠a Rodr√≠guez', 'Jos√© Mart√≠n', 'Laura S√°nchez']
    regiones = ['Norte', 'Sur', 'Este', 'Oeste', 'Centro']
    
    # Generar datos
    data = []
    for _ in range(n_records):
        fecha = start_date + timedelta(days=random.randint(0, 365))
        producto = random.choice(productos)
        categoria = random.choice(categorias)
        precio = round(random.uniform(50, 2000), 2)
        cantidad = random.randint(1, 10)
        vendedor = random.choice(vendedores)
        region = random.choice(regiones)
        
        data.append({
            'fecha': fecha,
            'producto': producto,
            'categoria': categoria,
            'precio': precio,
            'cantidad': cantidad,
            'vendedor': vendedor,
            'region': region,
            'total': precio * cantidad
        })
    
    return pd.DataFrame(data)

# Generar y guardar datos
df_ventas = generar_datos_ventas()
df_ventas.to_csv('ventas_datos.csv', index=False)
print("‚úÖ Dataset generado y guardado como 'ventas_datos.csv'")

# An√°lisis exploratorio
print("\nüìä AN√ÅLISIS EXPLORATORIO DE DATOS")
print("=" * 50)

# 1. Resumen estad√≠stico
print("\n1. Resumen Estad√≠stico:")
print(df_ventas.describe())

# 2. Ventas por mes y regi√≥n
df_ventas['mes'] = df_ventas['fecha'].dt.to_period('M')
ventas_mes_region = df_ventas.groupby(['mes', 'region'])['total'].sum().unstack()
print("\n2. Ventas por Mes y Regi√≥n:")
print(ventas_mes_region.head())

# 3. Top 10 productos
top_productos = df_ventas.groupby('producto')['cantidad'].sum().sort_values(ascending=False).head(10)
print("\n3. Top 10 Productos M√°s Vendidos:")
print(top_productos)

# 4. Mejor vendedor
mejor_vendedor = df_ventas.groupby('vendedor')['total'].sum().sort_values(ascending=False)
print("\n4. Performance por Vendedor:")
print(mejor_vendedor)

# 5. An√°lisis estacional
ventas_mensuales = df_ventas.groupby('mes')['total'].sum()
print("\n5. Ventas Mensuales (Estacionalidad):")
print(ventas_mensuales)

# 6. Correlaci√≥n precio-cantidad
correlacion = df_ventas['precio'].corr(df_ventas['cantidad'])
print(f"\n6. Correlaci√≥n Precio-Cantidad: {correlacion:.3f}")
```

</details>

### ü•à Ejercicio 2: Visualizaci√≥n de Datos

#### Objetivo
Crear visualizaciones profesionales con matplotlib y seaborn.

#### Prompts para Copilot
```python
# Crear dashboard de visualizaciones para datos de ventas:
# 1. Gr√°fico de l√≠neas: ventas mensuales con tendencia
# 2. Gr√°fico de barras: ventas por regi√≥n
# 3. Heatmap: correlaci√≥n entre variables num√©ricas
# 4. Box plot: distribuci√≥n de precios por categor√≠a
# 5. Gr√°fico circular: participaci√≥n de cada vendedor
# Usar estilo profesional con colores corporativos
```

## üåê Secci√≥n 2: APIs Web con FastAPI

### ü•á Ejercicio 3: API de An√°lisis de Datos

#### Objetivo
Crear una API que exponga an√°lisis de datos usando FastAPI.

#### Estructura del Proyecto
```bash
# Crear estructura del proyecto
mkdir analisis_api
cd analisis_api
mkdir app models services

# Estructura sugerida:
# analisis_api/
# ‚îú‚îÄ‚îÄ app/
# ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
# ‚îÇ   ‚îú‚îÄ‚îÄ main.py
# ‚îÇ   ‚îú‚îÄ‚îÄ models/
# ‚îÇ   ‚îú‚îÄ‚îÄ services/
# ‚îÇ   ‚îî‚îÄ‚îÄ routers/
# ‚îú‚îÄ‚îÄ data/
# ‚îî‚îÄ‚îÄ requirements.txt
```

#### Prompt Principal
```python
# API FastAPI para an√°lisis de datos de ventas
# Endpoints necesarios:
# GET /ventas/resumen - estad√≠sticas generales
# GET /ventas/por-region - ventas agrupadas por regi√≥n
# GET /ventas/top-productos - productos m√°s vendidos
# GET /ventas/vendedores - performance de vendedores
# POST /ventas/filtrar - filtrar ventas por criterios
# Incluir modelos Pydantic, manejo de errores, documentaci√≥n autom√°tica
```

#### Implementaci√≥n Paso a Paso

**1. Modelos Pydantic:**
```python
# models/ventas.py
# Modelos Pydantic para validaci√≥n de datos de ventas
# VentaModel: representar una venta individual
# FiltroVentas: modelo para filtros de b√∫squeda
# ResumenVentas: modelo para respuesta de resumen
# Incluir validaciones apropiadas para cada campo
```

**2. Servicios de An√°lisis:**
```python
# services/analisis_service.py
# Servicio para operaciones de an√°lisis de datos
# M√©todos: obtener_resumen, analizar_por_region, top_productos, performance_vendedores
# Usar pandas para procesamiento de datos
# Implementar cache para consultas frecuentes
```

**3. Rutas de la API:**
```python
# routers/ventas_router.py
# Router FastAPI con todos los endpoints de ventas
# Incluir documentaci√≥n detallada para cada endpoint
# Manejo de errores HTTP apropiados
# Validaci√≥n de par√°metros de entrada
```

### ü•à Ejercicio 4: Autenticaci√≥n y Middleware

#### Prompt para Autenticaci√≥n
```python
# Implementar autenticaci√≥n JWT en FastAPI
# Crear middleware para validar tokens
# Endpoints: /auth/login, /auth/register, /auth/refresh
# Proteger endpoints de an√°lisis con decorador de autenticaci√≥n
# Usar passlib para hash de contrase√±as
```

## ü§ñ Secci√≥n 3: Machine Learning

### ü•á Ejercicio 5: Predicci√≥n de Ventas

#### Objetivo
Implementar un modelo predictivo usando scikit-learn.

#### Dataset y Preprocessing
```python
# Preparaci√≥n de datos para machine learning:
# 1. Feature engineering: extraer caracter√≠sticas de fechas
# 2. Encoding categ√≥rico para variables no num√©ricas
# 3. Normalizaci√≥n de caracter√≠sticas num√©ricas
# 4. Divisi√≥n train/test estratificada
# 5. Manejo de valores faltantes si existen
```

#### Modelos a Implementar
```python
# Implementar y comparar m√∫ltiples modelos:
# 1. Linear Regression para predicci√≥n de ventas
# 2. Random Forest para clasificaci√≥n de productos exitosos
# 3. K-Means para segmentaci√≥n de clientes
# 4. Time Series forecasting con ARIMA
# Incluir evaluaci√≥n de modelos con m√©tricas apropiadas
```

#### Prompt Espec√≠fico
```python
# Sistema completo de predicci√≥n de ventas:
# 1. Clase ModeloVentas con m√©todos train, predict, evaluate
# 2. Pipeline de preprocessing autom√°tico
# 3. Validaci√≥n cruzada para selecci√≥n de hiperpar√°metros
# 4. Guardado y carga de modelos entrenados
# 5. API endpoint para hacer predicciones en tiempo real
```

### ü•à Ejercicio 6: An√°lisis de Sentimientos

#### Objetivo
Implementar an√°lisis de sentimientos de reviews de productos.

#### Prompt para Dataset
```python
# Generar dataset sint√©tico de reviews de productos:
# 1000 reviews con texto, calificaci√≥n (1-5), producto, fecha
# Reviews realistas tanto positivos como negativos
# Incluir variedad de productos tecnol√≥gicos
# Balancear sentimientos positivos, neutrales y negativos
```

#### Implementaci√≥n ML
```python
# Pipeline completo de an√°lisis de sentimientos:
# 1. Preprocessing de texto: limpieza, tokenizaci√≥n, stemming
# 2. Feature extraction: TF-IDF vectorizaci√≥n
# 3. Modelo de clasificaci√≥n: Naive Bayes o SVM
# 4. Evaluaci√≥n con matriz de confusi√≥n y m√©tricas
# 5. API endpoint para clasificar nuevas reviews
```

## üõ†Ô∏è Secci√≥n 4: Automatizaci√≥n y Scripts

### ü•á Ejercicio 7: Web Scraping Automatizado

#### Objetivo
Crear scripts de automatizaci√≥n para recopilaci√≥n de datos.

#### Prompt para Web Scraping
```python
# Script de web scraping para precios de productos:
# 1. Usar requests y BeautifulSoup
# 2. Scraper para m√∫ltiples sitios e-commerce
# 3. Manejo de rate limiting y headers apropiados
# 4. Guardado de datos en base de datos SQLite
# 5. Sistema de logging para monitoreo
# 6. Ejecuci√≥n programada con schedule
```

### ü•à Ejercicio 8: Generaci√≥n de Reportes

#### Objetivo
Automatizar la generaci√≥n de reportes empresariales.

#### Prompts Espec√≠ficos
```python
# Sistema de reportes automatizado:
# 1. Generador de reportes PDF con matplotlib y reportlab
# 2. Templates de reportes para diferentes departamentos
# 3. Env√≠o autom√°tico por email con schedule
# 4. Dashboard web con Streamlit para visualizaci√≥n interactiva
# 5. Integraci√≥n con Google Sheets para datos en tiempo real
```

## üèÜ Proyecto Final: Plataforma de Analytics

### Objetivo
Crear una plataforma completa de analytics con todas las tecnolog√≠as aprendidas.

### Arquitectura Sugerida
```
analytics_platform/
‚îú‚îÄ‚îÄ backend/                 # FastAPI backend
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îî‚îÄ‚îÄ ml_models/
‚îú‚îÄ‚îÄ data_processing/         # Scripts de procesamiento
‚îÇ   ‚îú‚îÄ‚îÄ scrapers/
‚îÇ   ‚îú‚îÄ‚îÄ etl/
‚îÇ   ‚îî‚îÄ‚îÄ ml_training/
‚îú‚îÄ‚îÄ frontend/               # Dashboard web (opcional)
‚îÇ   ‚îî‚îÄ‚îÄ streamlit_app.py
‚îú‚îÄ‚îÄ notebooks/              # Jupyter notebooks para an√°lisis
‚îú‚îÄ‚îÄ data/                   # Datasets
‚îú‚îÄ‚îÄ reports/                # Reportes generados
‚îî‚îÄ‚îÄ requirements.txt
```

### Funcionalidades Requeridas

1. **Backend API:**
   - Endpoints para CRUD de datos
   - An√°lisis estad√≠sticos en tiempo real
   - Predicciones ML como servicio
   - Autenticaci√≥n y autorizaci√≥n

2. **Procesamiento de Datos:**
   - ETL automatizado
   - Web scraping programado
   - Feature engineering autom√°tico
   - Entrenamiento continuo de modelos

3. **Visualizaci√≥n:**
   - Dashboard interactivo
   - Reportes PDF automatizados
   - Alertas por email/Slack
   - M√©tricas en tiempo real

### Prompts de Inicio para el Proyecto Final

1. **Arquitectura General:**
```
"Dise√±a la arquitectura completa de una plataforma de analytics empresarial con FastAPI, incluye estructura de carpetas, dependencias y flujo de datos"
```

2. **Backend Setup:**
```
"Crea el esqueleto de FastAPI con estructura modular para analytics: routers, services, models, database, y configuraci√≥n"
```

3. **Pipeline ML:**
```
"Implementa pipeline completo de machine learning: ingesta de datos, preprocessing, entrenamiento, evaluaci√≥n y deployment"
```

## üìö Buenas Pr√°cticas con Python y Copilot

### 1. Docstrings y Type Hints
```python
# ‚úÖ Copilot genera mejor c√≥digo con type hints
from typing import List, Dict, Optional
import pandas as pd

def analizar_ventas(
    df: pd.DataFrame, 
    fecha_inicio: Optional[str] = None,
    fecha_fin: Optional[str] = None
) -> Dict[str, float]:
    """
    Analiza datos de ventas en un per√≠odo espec√≠fico.
    
    Args:
        df: DataFrame con datos de ventas
        fecha_inicio: Fecha de inicio del an√°lisis (YYYY-MM-DD)
        fecha_fin: Fecha de fin del an√°lisis (YYYY-MM-DD)
    
    Returns:
        Diccionario con m√©tricas de ventas analizadas
    """
    # Copilot generar√° implementaci√≥n m√°s precisa
    pass
```

### 2. Manejo de Errores
```python
# Prompt: "Implementar manejo robusto de errores para API de datos"
from fastapi import HTTPException
import logging

async def procesar_datos(archivo: str):
    try:
        # Copilot agregar√° validaciones apropiadas
        pass
    except FileNotFoundError:
        logging.error(f"Archivo no encontrado: {archivo}")
        raise HTTPException(status_code=404, detail="Archivo no encontrado")
    except pd.errors.EmptyDataError:
        logging.error("El archivo est√° vac√≠o")
        raise HTTPException(status_code=400, detail="Datos inv√°lidos")
```

### 3. Testing
```python
# Prompt: "Crear tests unitarios completos para funciones de an√°lisis de datos"
import pytest
import pandas as pd
from unittest.mock import Mock, patch

def test_analizar_ventas():
    # Copilot generar√° casos de prueba apropiados
    pass

def test_api_endpoints():
    # Tests para endpoints FastAPI
    pass
```

## üîß Herramientas de Desarrollo

### Jupyter Notebooks con Copilot
```python
# En Jupyter, Copilot ayuda con:
# 1. An√°lisis exploratorio de datos paso a paso
# 2. Visualizaciones interactivas
# 3. Experimentaci√≥n con modelos ML
# 4. Documentaci√≥n autom√°tica de an√°lisis
```

### Debugging Efectivo
```python
# T√©cnicas de debugging con Copilot:
# 1. Usar logging estructurado
# 2. Implementar assertions en puntos cr√≠ticos
# 3. Crear funciones de validaci√≥n de datos
# 4. Usar breakpoints estrat√©gicos en VS Code
```

## üìä M√©tricas y Monitoreo

### Prompts para Monitoreo
```python
# Sistema de monitoreo para aplicaciones Python:
# 1. M√©tricas de performance con time/memory profiling
# 2. Health checks para APIs
# 3. Alertas autom√°ticas por email/Slack
# 4. Dashboard de m√©tricas en tiempo real
# 5. Logs estructurados con nivel apropiado
```

## üéØ Siguiente Paso

Despu√©s de completar los ejercicios de Python, puedes continuar con:
- **[React - Desarrollo Frontend](../react/README.md)**
- **[C# - Desarrollo Backend](../csharp/README.md)** (si no lo completaste)

## üìö Recursos Adicionales

- [Documentaci√≥n de FastAPI](https://fastapi.tiangolo.com/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)
- [Python con GitHub Copilot - Mejores Pr√°cticas](../docs/best-practices.md)

¬°Sigue experimentando con las capacidades de GitHub Copilot en Python!